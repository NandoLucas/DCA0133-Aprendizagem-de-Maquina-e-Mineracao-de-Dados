# -*- coding: utf-8 -*-
"""Perceptron_Aprox_Func.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fqu-XtlACy4bDEGSonSNVTYzFzFkMh_l

### Utilize uma rede neural perceptron de múltiplas camadas para aproximar a função abaixo.

- Apresente um gráfico com a curva da função analítica e a curva da função aproximada pela rede neural.
- Apresente também a curva da função custo no treinamento e a curva do erro
médio quadrado com relação ao o conjunto de validação.
- Procure definir a arquitetura da rede neural perceptron, isto é, o número de entradas, o número de neurônios em cada camada e o número de neurônios camada de saída.
- Observações: como se trata de um problema de aproximação de funções, considere a camada de saída do tipo linear puro.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
from mpl_toolkits.mplot3d import Axes3D
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

# Gerar dados
x1 = np.linspace(-5, 5, 100)
x2 = np.linspace(-5, 5, 100)
X1, X2 = np.meshgrid(x1, x2)
Y = X1**2 + X2**2 + 2*X1*X2 + np.cos(X1 + X2) - 1

# Flatten the data for training
X = np.vstack([X1.ravel(), X2.ravel()]).T
y = Y.ravel()

print("Quantidade de dados em x1:", len(x1))
print("Demonstração dos dados em x1:", x1)
print("Quantidade de dados em x2:", len(x2))
print("Demonstração dos dados em x2:", x2)

# Dividir dados em conjuntos de treinamento e validação
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Construir a rede neural
model = Sequential([
    Dense(64, activation='relu', input_shape=(2,)),
    Dense(32, activation='relu'),
    Dense(32, activation='relu'),
    Dense(16, activation='relu'),
    Dense(4, activation='relu'),
    Dense(1, activation='linear')
])

model.compile(
    loss='mean_squared_error',
    optimizer='adam',
)

# Treinar o modelo e guardar a função custo de validação em cada iteração
history = model.fit(X_train, y_train, epochs=100, validation_data=(X_val, y_val))

# Predições
y_pred = model.predict(X)

# Plotar a função analítica e a função aproximada
fig = plt.figure(figsize=(18, 6))

# Gráfico da função analítica
ax = fig.add_subplot(1, 3, 1, projection='3d')
ax.plot_surface(X1, X2, Y, cmap='viridis')
ax.set_title('Função analítica')

# Gráfico da função aproximada
Y_pred = y_pred.reshape(X1.shape)
ax = fig.add_subplot(1, 3, 2, projection='3d')
ax.plot_surface(X1, X2, Y_pred, cmap='viridis')
ax.set_title('Função aproximada pela MLP')

# Plotar a curva de custo durante o treinamento
plt.figure()
plt.plot(history.history['loss'], label='Treinamento')
plt.plot(history.history['val_loss'], label='Validação')
plt.legend()
plt.title('Função Custo durante o Treinamento')

plt.tight_layout()
plt.show()

# Plotar a curva de erro médio quadrado no treinamento e na validação
plt.figure()
plt.plot(history.history['loss'], label='Erro Médio Quadrado no Treinamento')
plt.plot(history.history['val_loss'], label='Erro Médio Quadrado na Validação')
plt.legend()
plt.title('Erro Médio Quadrado no Treinamento e na Validação')
plt.xlabel('Épocas')
plt.ylabel('Erro Médio Quadrado')
plt.show()

"""### Conclusão
- Foi possível verificar o quão precisa é a MLP para aproximação de funções;
- O erro diminui com o número de épocas rapidamente;
- Como a curva de custo foi definida como a 'mean_squared_error' na qual é o erro médio quadrado, terá o mesmo resultado se plotar separadamente;
"""